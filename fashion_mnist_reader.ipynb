{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/fashion/train-labels-idx1-ubyte.gz\n",
      "data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Read Fashion MNIST dataset\n",
    "\n",
    "import mnist_reader\n",
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "# Your code goes here . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 0 0 ... 3 0 5]\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(784,)\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "# print(X_test, y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plot the first image in the dataset\n",
    "print(X_train[0].shape)\n",
    "\n",
    "import numpy as np\n",
    "# importing one hot encoder from sklearn \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = encoder.fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train / 255)\n",
    "X_test = (X_test / 255)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derv(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "def softmax(s):\n",
    "    exps = np.exp(s - np.max(s, axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    res = pred - real\n",
    "    return res/n_samples\n",
    "\n",
    "def error(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
    "    loss = np.sum(logp)/n_samples\n",
    "    return loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               173184    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 174,554\n",
      "Trainable params: 174,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.4492 - acc: 0.8382 - val_loss: 0.3875 - val_acc: 0.8585\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.3056 - acc: 0.8897 - val_loss: 0.3277 - val_acc: 0.8837\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2603 - acc: 0.9063 - val_loss: 0.2905 - val_acc: 0.8981\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2287 - acc: 0.9165 - val_loss: 0.2783 - val_acc: 0.8976\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2053 - acc: 0.9247 - val_loss: 0.2941 - val_acc: 0.8963\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1848 - acc: 0.9326 - val_loss: 0.2806 - val_acc: 0.9011\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.1664 - acc: 0.9399 - val_loss: 0.2784 - val_acc: 0.9067\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1513 - acc: 0.9456 - val_loss: 0.2936 - val_acc: 0.9076\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1370 - acc: 0.9511 - val_loss: 0.2894 - val_acc: 0.9066\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1234 - acc: 0.9557 - val_loss: 0.3077 - val_acc: 0.9051\n",
      "10000/10000 [==============================] - 0s 24us/step\n",
      "Test loss: 0.3076509069502354\n",
      "Test accuracy: 0.9051\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "\n",
    "X_train_conv = np.copy(X_train).reshape(60000,28,28,1)\n",
    "X_test_conv = np.copy(X_test).reshape(10000,28,28,1)\n",
    "\n",
    "filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "convolutional_model = Sequential()\n",
    "convolutional_model.add(Conv2D(filters, filter_size, input_shape=(28, 28, 1)))\n",
    "convolutional_model.add(MaxPooling2D(pool_size = pool_size))\n",
    "convolutional_model.add(Flatten())\n",
    "convolutional_model.add(Dense(128, activation='sigmoid'))\n",
    "convolutional_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "convolutional_model.summary()\n",
    "\n",
    "# keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "convolutional_model.compile(optimizer=Adadelta(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "convolutional_model.fit(X_train_conv, y_train, epochs=10, validation_data=(X_train_conv, y_test))\n",
    "\n",
    "scores = convolutional_model.evaluate(X_test_conv, y_test,)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print(\"\\n%s: %.2f%%\" % (convolutional_model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim= X_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adadelta(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.neurons = 128\n",
    "        self.learning_rate = 0.5\n",
    "        self.theta1 = np.random.randn(self.x.shape[1], self.neurons)\n",
    "        self.bias1 = np.zeros((1, self.neurons))\n",
    "        self.theta2 = np.random.randn(self.neurons, self.y.shape[1])\n",
    "        self.bias2 = np.zeros((1, self.y.shape[1]))\n",
    "       \n",
    "       \n",
    "        \n",
    "    # y = softmax(W2 sigmoid(w1x + b1) + b2)\n",
    "    def forward_feeding(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.x, self.theta1) + self.bias1)\n",
    "        self.output = softmax(np.dot(self.layer1, self.theta2) + self.bias2)\n",
    "             \n",
    "        \n",
    "    def back_propagation(self, epoch_count):\n",
    "        loss = error(self.output, self.y)\n",
    "        if(epoch_count % 50 == 0):\n",
    "             print('Error, epoch_count:', loss, epoch_count)\n",
    "        \n",
    "        a2_delta = cross_entropy(self.output, self.y) # theta2\n",
    "              \n",
    "        z1_delta = np.dot(a2_delta, self.theta2.T)\n",
    "        a1_delta = z1_delta * sigmoid_derv(self.layer1) # theta1\n",
    "        \n",
    "        self.theta2 -= self.learning_rate * np.dot(self.layer1.T, a2_delta)\n",
    "        self.bias2 -= self.learning_rate * np.sum(a2_delta, axis=0, keepdims=True)\n",
    "        \n",
    "        self.theta1 -= self.learning_rate * np.dot(self.x.T, a1_delta)\n",
    "        self.bias1 -= self.learning_rate * np.sum(a1_delta, axis=0)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        self.x = data\n",
    "        self.forward_feeding()\n",
    "        return self.output.argmax()\n",
    "        \n",
    "\n",
    "model = NeuralNetwork(X_train, y_train)\n",
    "epochs = 300\n",
    "for x in range(epochs):\n",
    "    model.forward_feeding()\n",
    "    model.back_propagation(x)\n",
    "\n",
    "def get_acc(x, y):\n",
    "    acc = 0\n",
    "    for xx,yy in zip(x, y):\n",
    "        s = model.predict(xx)\n",
    "        if s == np.argmax(yy):\n",
    "            acc +=1\n",
    "    return acc/len(x)*100\n",
    "\n",
    "print(\"Training accuracy : \", get_acc(X_train, np.array(y_train)))\n",
    "print(\"Test accuracy : \", get_acc(X_test, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
